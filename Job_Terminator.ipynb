{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "2jAf1SQLBk7y",
    "outputId": "411f49a2-5b72-4788-c997-83ba41e7fb4c"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.llms import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vn3hyveafmDB",
    "outputId": "919d22ff-1632-477d-d4a2-46f59cd882be"
   },
   "outputs": [],
   "source": [
    "# load and read data.json\n",
    "\n",
    "with open('data.json') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNgzlU9o9Apq",
    "outputId": "b4acc92a-82aa-481e-b152-ee5dcb4c967d"
   },
   "outputs": [],
   "source": [
    "# load the env.txt that contains the OPENAI key\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB_YDDxoMSBR",
    "outputId": "5c541321-4dae-4fd0-94e6-f3d31985b83f"
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[39m=\u001b[39m OpenAI(\n\u001b[1;32m      2\u001b[0m     openai_api_key\u001b[39m=\u001b[39mopenai_api_key,\n\u001b[1;32m      3\u001b[0m     model \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo-instruct\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m textType \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:171\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     emit_warning()\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model ='gpt-3.5-turbo-instruct'\n",
    ")\n",
    "textType = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = \"Your narrative effectively captures the tension, excitement, and triumph of your debate team's journey. Here are some strengths: Engaging Opening: The beginning of your story draws the reader in by describing the nervous atmosphere and the stakes involved in the debate competition. It effectively sets the stage for the rest of the narrative. Well-Developed Characters: You provide a glimpse into the personalities and emotions of the characters, particularly the protagonist and their team. The coach's reassurance, the competitors' condescending behavior, and the team's reaction after the win all contribute to a well-rounded story. Effective Use of Descriptive Language: Your use of descriptive language helps paint a vivid picture of the scene. The physical sensations of anxiety, the discomfort of the formal uniform, and the tightness of the tie contribute to the reader's immersion in the story. Narrative Arc and Resolution: The narrative follows a clear arc from nervous anticipation to the surprising victory and the subsequent confrontation with the opposing team. The resolution, where the underdogs stand up for themselves, adds a satisfying conclusion to the story. Themes of Resilience and Triumph: The story effectively conveys themes of resilience, determination, and triumph in the face of adversity. The underdog narrative is a powerful and relatable element that many readers can connect with. Areas for Consideration: Show, Don't Tell: While you do a good job describing the physical sensations and emotions, consider showing more of the characters' internal thoughts and reactions. This can enhance the reader's connection to the characters and their experiences. Dialogue Punctuation: Pay attention to the punctuation of your dialogue. For example, use commas or periods inside quotation marks, and make sure each speaker's dialogue is on a new line. Diverse Sentence Structure: While your sentence structure is generally varied, consider experimenting with different sentence lengths and structures to add more dynamic flow to the narrative. Deepening Emotional Impact: Explore opportunities to deepen the emotional impact of certain moments, such as the announcement of the winner. Delve into the characters' thoughts and feelings to convey the magnitude of their emotions. Overall, your narrative is compelling and successfully conveys the triumph of the underdog team. With a few adjustments, you can enhance certain elements to create an even more immersive and emotionally resonant experience for the reader.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yjU2ImqkieI5"
   },
   "outputs": [],
   "source": [
    "narrative_custom_criterion = {\n",
    "    \"grammar\": (\n",
    "        \"Does the essay follow proper grammar rules?\"\n",
    "        \"Does the essay follow proper pronunciation rules?\"\n",
    "        \"Does the output use a robust dictionary of vocabulary?\"\n",
    "    ),\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"description\":\"Does the output include sensory words?\",\n",
    "    \"characters\":\"Does the output capture the personality or emotions of the characters in the story?\",\n",
    "    \"openings\": \"Does the output have an engaging opening?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentative_custom_criterion = {\n",
    "    \"grammar\": \"Does the output follow proper grammar, spelling, and pronunciation rules?\",\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"vocabulary\":\"Does the output use a robust dictionary of vocabulary?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"attention\":\"Does the output capture your attention?\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "uNa_hZLw_noy",
    "outputId": "6c7716ec-b75c-4e14-f28f-8dc5bb33e365"
   },
   "outputs": [],
   "source": [
    "def model(textType):\n",
    "        if textType.lower() == \"narrative\":\n",
    "                custom_criterion = narrative_custom_criterion\n",
    "        else:\n",
    "                custom_criterion = narrative_custom_criterion\n",
    "\n",
    "        evalReso = load_evaluator(\n",
    "        EvaluatorType.CRITERIA,\n",
    "        criteria = custom_criterion,\n",
    "        llm=llm\n",
    "        )\n",
    "\n",
    "        question = \"\"\n",
    "        sampleFeedback = \"\"\n",
    "        evalResult = evalReso.evaluate_strings(prediction=sampleFeedback, input=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "NRnBjgYuMvyh"
   },
   "outputs": [],
   "source": [
    "def eval(source,essay_type,essay):\n",
    "    source,essay_type,essay = source,essay_type,essay\n",
    "    model(essay_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "narrative\n",
      "!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/j5/jkmc_7cj44bbbdvx7swyqpch0000gn/T/ipykernel_8316/2462729634.py\", line 4, in eval\n",
      "    model(essay_type)\n",
      "  File \"/var/folders/j5/jkmc_7cj44bbbdvx7swyqpch0000gn/T/ipykernel_8316/3441321291.py\", line 7, in model\n",
      "    evalReso = evaluator.evaluate_strings(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 218, in evaluate_strings\n",
      "    self._check_evaluation_args(reference=reference, input=input)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 126, in _check_evaluation_args\n",
      "    raise ValueError(f\"{self.__class__.__name__} requires a reference string.\")\n",
      "ValueError: ContextQAEvalChain requires a reference string.\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    eval,\n",
    "    [\"text\",\"text\",\"text\"],\n",
    "    \"text\",\n",
    "    title = f'O level  Essay Grader'\n",
    ")\n",
    "demo.launch()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
