{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "2jAf1SQLBk7y",
    "outputId": "411f49a2-5b72-4788-c997-83ba41e7fb4c"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNgzlU9o9Apq",
    "outputId": "b4acc92a-82aa-481e-b152-ee5dcb4c967d"
   },
   "outputs": [],
   "source": [
    "# load the env.txt that contains the OPENAI key\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB_YDDxoMSBR",
    "outputId": "5c541321-4dae-4fd0-94e6-f3d31985b83f"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model ='gpt-3.5-turbo-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_feedback = \"Your narrative effectively captures the tension, excitement, and triumph of your debate team's journey. Here are some strengths: Engaging Opening: The beginning of your story draws the reader in by describing the nervous atmosphere and the stakes involved in the debate competition. It effectively sets the stage for the rest of the narrative. Well-Developed Characters: You provide a glimpse into the personalities and emotions of the characters, particularly the protagonist and their team. The coach's reassurance, the competitors' condescending behavior, and the team's reaction after the win all contribute to a well-rounded story. Effective Use of Descriptive Language: Your use of descriptive language helps paint a vivid picture of the scene. The physical sensations of anxiety, the discomfort of the formal uniform, and the tightness of the tie contribute to the reader's immersion in the story. Narrative Arc and Resolution: The narrative follows a clear arc from nervous anticipation to the surprising victory and the subsequent confrontation with the opposing team. The resolution, where the underdogs stand up for themselves, adds a satisfying conclusion to the story. Themes of Resilience and Triumph: The story effectively conveys themes of resilience, determination, and triumph in the face of adversity. The underdog narrative is a powerful and relatable element that many readers can connect with. Areas for Consideration: Show, Don't Tell: While you do a good job describing the physical sensations and emotions, consider showing more of the characters' internal thoughts and reactions. This can enhance the reader's connection to the characters and their experiences. Dialogue Punctuation: Pay attention to the punctuation of your dialogue. For example, use commas or periods inside quotation marks, and make sure each speaker's dialogue is on a new line. Diverse Sentence Structure: While your sentence structure is generally varied, consider experimenting with different sentence lengths and structures to add more dynamic flow to the narrative. Deepening Emotional Impact: Explore opportunities to deepen the emotional impact of certain moments, such as the announcement of the winner. Delve into the characters' thoughts and feelings to convey the magnitude of their emotions. Overall, your narrative is compelling and successfully conveys the triumph of the underdog team. With a few adjustments, you can enhance certain elements to create an even more immersive and emotionally resonant experience for the reader.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yjU2ImqkieI5"
   },
   "outputs": [],
   "source": [
    "narrative_custom_criterion = {\n",
    "    \"grammar\": (\n",
    "        \"Does the essay follow proper grammar rules?\"\n",
    "        \"Does the essay follow proper pronunciation rules?\"\n",
    "        \"Does the output use a robust dictionary of vocabulary?\"\n",
    "    ),\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"description\":\"Does the output include sensory words?\",\n",
    "    \"characters\":\"Does the output capture the personality or emotions of the characters in the story?\",\n",
    "    \"openings\": \"Does the output have an engaging opening?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentative_custom_criterion = {\n",
    "    \"grammar\": \"Does the output follow proper grammar, spelling, and pronunciation rules?\",\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"vocabulary\":\"Does the output use a robust dictionary of vocabulary?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"attention\":\"Does the output capture your attention?\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "uNa_hZLw_noy",
    "outputId": "6c7716ec-b75c-4e14-f28f-8dc5bb33e365"
   },
   "outputs": [],
   "source": [
    "def model(essay_question,essay_type,essay):\n",
    "        if essay_type.lower() == \"narrative\":\n",
    "                custom_criterion = narrative_custom_criterion\n",
    "        elif essay_type.lower() == \"argumentative\":\n",
    "                custom_criterion = argumentative_custom_criterion\n",
    "\n",
    "        evalReso = load_evaluator(\n",
    "        evaluator=EvaluatorType.CRITERIA,\n",
    "        criteria=custom_criterion,\n",
    "        llm=llm,\n",
    "        )\n",
    "        prompt = \"You are a English O'level Cambridge Grader for an essay, please give feedback. This is the question: \" + essay_question + \" . give feedback to this essay: \" + essay\n",
    "        evalResult = evalReso.evaluate_strings(prediction=prompt, input=essay)\n",
    "\n",
    "        return evalResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "NRnBjgYuMvyh"
   },
   "outputs": [],
   "source": [
    "def eval(source,essay_type,essay):\n",
    "    source,essay_type,essay = source,essay_type,essay\n",
    "    return model(source,essay_type,essay)[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/j5/jkmc_7cj44bbbdvx7swyqpch0000gn/T/ipykernel_12456/143752376.py\", line 3, in eval\n",
      "    return model(source,essay_type,essay)[\"response\"]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/j5/jkmc_7cj44bbbdvx7swyqpch0000gn/T/ipykernel_12456/1582855211.py\", line 13, in model\n",
      "    evalResult = evalReso.evaluate_strings(prediction=prompt, input=essay)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/evaluation/schema.py\", line 219, in evaluate_strings\n",
      "    return self._evaluate_strings(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/evaluation/criteria/eval_chain.py\", line 447, in _evaluate_strings\n",
      "    result = self(\n",
      "             ^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 145, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py\", line 363, in __call__\n",
      "    return self.invoke(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py\", line 162, in invoke\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/chains/base.py\", line 156, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py\", line 103, in _call\n",
      "    response = self.generate([inputs], run_manager=run_manager)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain/chains/llm.py\", line 115, in generate\n",
      "    return self.llm.generate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 525, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 698, in generate\n",
      "    output = self._generate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 562, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_core/language_models/llms.py\", line 549, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/openai.py\", line 460, in _generate\n",
      "    response = completion_with_retry(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/langchain_community/llms/openai.py\", line 115, in completion_with_retry\n",
      "    return llm.client.create(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/openai/_utils/_utils.py\", line 271, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/openai/resources/completions.py\", line 560, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py\", line 1112, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py\", line 859, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/openai/_base_client.py\", line 949, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-Hu1iR***************************************Kpnn. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    eval,\n",
    "    [\"text\",\"text\",\"text\"],\n",
    "    \"text\",\n",
    "    title = f'O level  Essay Grader'\n",
    ")\n",
    "demo.launch()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
