{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "2jAf1SQLBk7y",
    "outputId": "411f49a2-5b72-4788-c997-83ba41e7fb4c"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.llms import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vn3hyveafmDB",
    "outputId": "919d22ff-1632-477d-d4a2-46f59cd882be"
   },
   "outputs": [],
   "source": [
    "# load and read data.json\n",
    "\n",
    "with open('data.json') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNgzlU9o9Apq",
    "outputId": "b4acc92a-82aa-481e-b152-ee5dcb4c967d"
   },
   "outputs": [],
   "source": [
    "# load the env.txt that contains the OPENAI key\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB_YDDxoMSBR",
    "outputId": "5c541321-4dae-4fd0-94e6-f3d31985b83f"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model ='gpt-3.5-turbo-instruct'\n",
    ")\n",
    "textType = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yjU2ImqkieI5"
   },
   "outputs": [],
   "source": [
    "narrative_custom_criterion = {\n",
    "    \"grammar\": (\n",
    "        \"Does the essay follow proper grammar rules?\"\n",
    "        \"Does the essay follow proper pronunciation rules?\"\n",
    "        \"Does the output use a robust dictionary of vocabulary?\"\n",
    "    ),\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"description\":\"Does the output include sensory words?\",\n",
    "    \"characters\":\"Does the output capture the personality or emotions of the characters in the story?\",\n",
    "    \"openings\": \"Does the output have an engaging opening?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentative_custom_criterion = {\n",
    "    \"grammar\": \"Does the output follow proper grammar, spelling, and pronunciation rules?\",\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"vocabulary\":\"Does the output use a robust dictionary of vocabulary?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"attention\":\"Does the output capture your attention?\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "uNa_hZLw_noy",
    "outputId": "6c7716ec-b75c-4e14-f28f-8dc5bb33e365"
   },
   "outputs": [],
   "source": [
    "if textType == \"Narrative\":\n",
    "        evaluator = load_evaluator(\"context_qa\", criteria=\"narrative_custom_criterion\", llm=llm)\n",
    "else:\n",
    "        evaluator = load_evaluator(\"context_qa\", criteria=\"argumentative_custom_criterion\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['context', 'query', 'result'], template=\"You are a teacher grading a quiz.\\nYou are given a question, the context the question is about, and the student's answer. You are asked to score the student's answer as either CORRECT or INCORRECT, based on the context.\\n\\nExample Format:\\nQUESTION: question here\\nCONTEXT: context the question is about here\\nSTUDENT ANSWER: student's answer here\\nGRADE: CORRECT or INCORRECT here\\n\\nGrade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. Begin! \\n\\nQUESTION: {query}\\nCONTEXT: {context}\\nSTUDENT ANSWER: {result}\\nGRADE:\") llm=OpenAI(client=<openai.resources.completions.Completions object at 0x10a59a590>, async_client=<openai.resources.completions.AsyncCompletions object at 0x126c9f410>, openai_api_key='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', openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ContextQAEvalChain requires a reference string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evalReso \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39mevaluate_strings(\n\u001b[1;32m      2\u001b[0m     prediction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are a Cambridge O-Level English teacher who is marking English Paper 1 Composition Scripts, please use the marking criterias to mark\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEvaluate this essay using\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/evaluation/schema.py:218\u001b[0m, in \u001b[0;36mStringEvaluator.evaluate_strings\u001b[0;34m(self, prediction, reference, input, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_strings\u001b[39m(\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    202\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Evaluate Chain or LLM output, based on optional input and label.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m        dict: The evaluation results containing the score or value.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_evaluation_args(reference\u001b[39m=\u001b[39mreference, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m)\n\u001b[1;32m    219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate_strings(\n\u001b[1;32m    220\u001b[0m         prediction\u001b[39m=\u001b[39mprediction, reference\u001b[39m=\u001b[39mreference, \u001b[39minput\u001b[39m\u001b[39m=\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/evaluation/schema.py:126\u001b[0m, in \u001b[0;36m_EvalArgsMixin._check_evaluation_args\u001b[0;34m(self, reference, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m     warn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip_input_warning)\n\u001b[1;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_reference \u001b[39mand\u001b[39;00m reference \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m requires a reference string.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[39melif\u001b[39;00m reference \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_reference:\n\u001b[1;32m    128\u001b[0m     warn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_skip_reference_warning)\n",
      "\u001b[0;31mValueError\u001b[0m: ContextQAEvalChain requires a reference string."
     ]
    }
   ],
   "source": [
    "evalReso = evaluator.evaluate_strings(\n",
    "    prediction=\"You are a Cambridge O-Level English teacher who is marking English Paper 1 Composition Scripts, please use the marking criterias to mark\",\n",
    "    input=\"Evaluate this essay using\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NRnBjgYuMvyh"
   },
   "outputs": [],
   "source": [
    "def eval(source,essay_type,essay):\n",
    "    source,essay_type,essay = source,essay_type,essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    eval,\n",
    "    [\"text\",\"text\",\"text\"],\n",
    "    \"text\",\n",
    "    title = f'O level  Essay Grader'\n",
    ")\n",
    "demo.launch()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
