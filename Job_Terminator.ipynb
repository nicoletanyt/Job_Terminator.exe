{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 560
    },
    "id": "2jAf1SQLBk7y",
    "outputId": "411f49a2-5b72-4788-c997-83ba41e7fb4c"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.llms import OpenAI\n",
    "import gradio as gr\n",
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vn3hyveafmDB",
    "outputId": "919d22ff-1632-477d-d4a2-46f59cd882be"
   },
   "outputs": [],
   "source": [
    "# load and read data.json\n",
    "\n",
    "with open('data.json') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_input(source,essay_type,essay):\n",
    "    source,essay_type,essay = source,essay_type,essay\n",
    "\n",
    "demo = gr.Interface(\n",
    "    eval_input,\n",
    "    [\"text\",\"text\",\"text\"],\n",
    "    None,\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNgzlU9o9Apq",
    "outputId": "b4acc92a-82aa-481e-b152-ee5dcb4c967d"
   },
   "outputs": [],
   "source": [
    "# load the env.txt that contains the OPENAI key\n",
    "\n",
    "load_dotenv('env.txt')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GB_YDDxoMSBR",
    "outputId": "5c541321-4dae-4fd0-94e6-f3d31985b83f"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model ='gpt-3.5-turbo-instruct'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yjU2ImqkieI5"
   },
   "outputs": [],
   "source": [
    "narrative_custom_criterion = {\n",
    "    \"grammar\": \"Does the output follow proper grammar, spelling, and pronunciation rules?\",\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"vocabulary\":\"Does the output use a robust dictionary of vocabulary?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"description\":\"Does the output include sensory words?\",\n",
    "    \"characters\":\"Does the output capture the personality or emotions of the characters in the story?\",\n",
    "    \"openings\": \"Does the output have an engaging opening?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "argumentative_custom_criterion = {\n",
    "    \"grammar\": \"Does the output follow proper grammar, spelling, and pronunciation rules?\",\n",
    "    \"coherence\": \"Does the output have a coherent flow of ideas?\",\n",
    "    \"vocabulary\":\"Does the output use a robust dictionary of vocabulary?\",\n",
    "    \"relevance\":\"Does the output fully address the task?\",\n",
    "    \"attention\":\"Does the output capture your attention?\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "uNa_hZLw_noy",
    "outputId": "6c7716ec-b75c-4e14-f28f-8dc5bb33e365"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "evaluator = load_evaluator(\"pairwise_string\", criteria=\"custom_criterion\", llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=ChatPromptTemplate(input_variables=['input', 'prediction', 'prediction_b'], partial_variables={'reference': '', 'criteria': 'For this evaluation, you should primarily consider the following criteria:\\ncustom_criterion'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\\'s instructions and answers \\the user\\'s question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['criteria', 'input', 'prediction', 'prediction_b'], template=\"{criteria}[User Question]\\n{input}\\n\\n[The Start of Assistant A's Answer]\\n{prediction}\\n[The End of Assistant A's Answer]\\n\\n[The Start of Assistant B's Answer]\\n{prediction_b}\\n[The End of Assistant B's Answer]\"))]) llm=OpenAI(client=<openai.resources.completions.Completions object at 0x12a2f9590>, async_client=<openai.resources.completions.AsyncCompletions object at 0x129b5de50>, openai_api_key='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', openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "print(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NRnBjgYuMvyh"
   },
   "outputs": [],
   "source": [
    "def eval(source,essay_type,essay):\n",
    "    source,essay_type,essay = source,essay_type,essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    eval,\n",
    "    [\"text\",\"text\",\"text\"],\n",
    "    \"text\",\n",
    "    title = f'O level Essay Grader'\n",
    ")\n",
    "demo.launch()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
